{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xarray as xr\n",
    "from src.field_topology.topology_utils import *\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# SETTINGS\n",
    "# --------------------------\n",
    "case = \"CPN\"\n",
    "step = 350000  # 115000 for base or 350000 for hnhv\n",
    "\n",
    "input_folder = f\"/Volumes/data_backup/mercury/extreme/High_HNHV/{case}_HNHV/10/out/\"\n",
    "ncfile = os.path.join(input_folder, f\"Amitis_{case}_HNHV_{step}.nc\")\n",
    "\n",
    "# input_folder = f\"/Volumes/data_backup/mercury/extreme/{case}_Base/05/out/\"\n",
    "# ncfile = os.path.join(input_folder, f\"Amitis_{case}_Base_{step}.nc\")\n",
    "\n",
    "output_folder = f\"/Users/danywaller/Projects/mercury/extreme/jfield_feature_classification/{case}/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Planet parameters\n",
    "RM = 2440.0          # Mercury radius [km]\n",
    "RC = 2400.0          # depth within conductive layer [km]\n",
    "\n",
    "plot_depth = RM\n",
    "\n",
    "# Seed settings\n",
    "n_lat = 60\n",
    "n_lon = n_lat*2\n",
    "max_steps = 5000\n",
    "h_step = 50.0\n",
    "surface_tol = 75.0\n",
    "\n",
    "max_lines = 250  # downsample trajectory points for plotting\n",
    "\n",
    "# Clustering settings\n",
    "clustering_method = \"hierarchical\"  # \"dbscan\" or \"hierarchical\"\n",
    "auto_select_clusters = True  # automatically find optimal number of clusters\n",
    "max_clusters_to_test = 10  # maximum number of clusters to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CREATE SEEDS ON SPHERE\n",
    "# --------------------------\n",
    "lats_surface = np.linspace(-90, 90, n_lat)\n",
    "lons_surface = np.linspace(-180, 180, n_lon)\n",
    "seeds = []\n",
    "for lat in lats_surface:\n",
    "    for lon in lons_surface:\n",
    "        phi = np.radians(lat)\n",
    "        theta = np.radians(lon)\n",
    "        x_s = plot_depth*np.cos(phi)*np.cos(theta)\n",
    "        y_s = plot_depth*np.cos(phi)*np.sin(theta)\n",
    "        z_s = plot_depth*np.sin(phi)\n",
    "        seeds.append(np.array([x_s, y_s, z_s]))\n",
    "seeds = np.array(seeds)\n",
    "\n",
    "# --------------------------\n",
    "# LOAD VECTOR FIELD FROM NETCDF\n",
    "# --------------------------\n",
    "def load_field(ncfile):\n",
    "    ds = xr.open_dataset(ncfile)\n",
    "    x = ds[\"Nx\"].values\n",
    "    y = ds[\"Ny\"].values\n",
    "    z = ds[\"Nz\"].values\n",
    "\n",
    "    # Extract fields (drop time dimension) and transpose:  Nz, Ny, Nx --> Nx, Ny, Nz\n",
    "    Jx = np.transpose(ds[\"Jx\"].isel(time=0).values, (2,1,0))\n",
    "    Jy = np.transpose(ds[\"Jy\"].isel(time=0).values, (2,1,0))\n",
    "    Jz = np.transpose(ds[\"Jz\"].isel(time=0).values, (2,1,0))\n",
    "    ds.close()\n",
    "    return x, y, z, Jx, Jy, Jz\n",
    "\n",
    "# --------------------------\n",
    "# FEATURE EXTRACTION\n",
    "# --------------------------\n",
    "def extract_trajectory_features(traj_fwd, traj_bwd):\n",
    "    \"\"\"\n",
    "    Extract geometric and topological features from a field line trajectory.\n",
    "    \"\"\"\n",
    "    full_traj = np.vstack([traj_bwd[::-1], traj_fwd])\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Spatial extent features\n",
    "    features.append(np.min(full_traj[:, 0]))      # min X\n",
    "    features.append(np.max(full_traj[:, 0]))      # max X\n",
    "    features.append(np.mean(full_traj[:, 0]))     # mean X\n",
    "\n",
    "    features.append(np.min(full_traj[:, 1]))      # min Y\n",
    "    features.append(np.max(full_traj[:, 1]))      # max Y\n",
    "    features.append(np.mean(full_traj[:, 1]))     # mean Y\n",
    "\n",
    "    features.append(np.min(full_traj[:, 2]))      # min Z\n",
    "    features.append(np.max(full_traj[:, 2]))      # max Z\n",
    "    features.append(np.mean(full_traj[:, 2]))     # mean Z\n",
    "\n",
    "    # Radial features\n",
    "    r = np.linalg.norm(full_traj, axis=1)\n",
    "    features.append(np.min(r))                     # min radial distance\n",
    "    features.append(np.max(r))                     # max radial distance\n",
    "    features.append(np.mean(r))                    # mean radial distance\n",
    "\n",
    "    # Shape features\n",
    "    features.append(np.std(full_traj[:, 0]))      # X spread\n",
    "    features.append(np.std(full_traj[:, 1]))      # Y spread\n",
    "    features.append(np.std(full_traj[:, 2]))      # Z spread\n",
    "\n",
    "    # Asymmetry features\n",
    "    features.append(np.abs(np.mean(full_traj[:, 2])))  # Z asymmetry\n",
    "\n",
    "    # Arc length\n",
    "    diffs = np.diff(full_traj, axis=0)\n",
    "    arc_length = np.sum(np.linalg.norm(diffs, axis=1))\n",
    "    features.append(arc_length)\n",
    "\n",
    "    # Tail vs dayside preference\n",
    "    features.append(np.sum(full_traj[:, 0] < 0) / len(full_traj))\n",
    "\n",
    "    # High latitude measure\n",
    "    features.append(np.sum(np.abs(full_traj[:, 2]) > RM) / len(full_traj))\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def compute_poincare_section(traj_fwd, traj_bwd, plane='xy', n_crossings=50):\n",
    "    \"\"\"\n",
    "    Compute Poincare section crossings for a field line trajectory.\n",
    "    \"\"\"\n",
    "    full_traj = np.vstack([traj_bwd[::-1], traj_fwd])\n",
    "\n",
    "    # Determine plane normal and coordinate indices\n",
    "    if plane == 'xy':\n",
    "        normal_idx = 2  # z-coordinate\n",
    "        plane_indices = [0, 1]  # x, y\n",
    "    elif plane == 'xz':\n",
    "        normal_idx = 1  # y-coordinate\n",
    "        plane_indices = [0, 2]  # x, z\n",
    "    else:  # 'yz'\n",
    "        normal_idx = 0  # x-coordinate\n",
    "        plane_indices = [1, 2]  # y, z\n",
    "\n",
    "    # Find crossings through plane\n",
    "    normal_coords = full_traj[:, normal_idx]\n",
    "    sign_changes = np.where(np.diff(np.sign(normal_coords)) != 0)[0]\n",
    "\n",
    "    if len(sign_changes) == 0:\n",
    "        indices = np.linspace(0, len(full_traj)-1, min(n_crossings, len(full_traj)), dtype=int)\n",
    "        return full_traj[indices][:, plane_indices]\n",
    "\n",
    "    # Interpolate crossing points\n",
    "    crossings = []\n",
    "    for idx in sign_changes[:n_crossings]:\n",
    "        p1 = full_traj[idx]\n",
    "        p2 = full_traj[idx + 1]\n",
    "        t = -p1[normal_idx] / (p2[normal_idx] - p1[normal_idx])\n",
    "        crossing = p1 + t * (p2 - p1)\n",
    "        crossings.append(crossing[plane_indices])\n",
    "\n",
    "    if len(crossings) == 0:\n",
    "        indices = np.linspace(0, len(full_traj)-1, min(n_crossings, len(full_traj)), dtype=int)\n",
    "        return full_traj[indices][:, plane_indices]\n",
    "\n",
    "    return np.array(crossings)\n",
    "\n",
    "\n",
    "def compute_persistent_homology_features(point_cloud, max_dimension=1, max_edge_length=None):\n",
    "    \"\"\"\n",
    "    Compute persistent homology using Vietoris-Rips complex.\n",
    "    Robust to degenerate cases (too few points, identical points, etc.)\n",
    "    \"\"\"\n",
    "    # Initialize default features\n",
    "    default_features = {\n",
    "        'h0_mean_persistence': 0.0,\n",
    "        'h0_max_persistence': 0.0,\n",
    "        'h0_num_components': 1,\n",
    "        'h1_num_holes': 0,\n",
    "        'h1_max_persistence': 0.0,\n",
    "        'h1_mean_persistence': 0.0,\n",
    "        'h1_sum_persistence': 0.0,\n",
    "        'h1_entropy': 0.0\n",
    "    }\n",
    "\n",
    "    # Default empty diagrams (proper shape for persim)\n",
    "    default_diagrams = [\n",
    "        np.array([[0, np.inf]]),  # H0: one infinite component\n",
    "        np.empty((0, 2))           # H1: no holes (MUST be 2D empty array)\n",
    "    ]\n",
    "\n",
    "    # Check minimum requirements\n",
    "    if len(point_cloud) < 4:\n",
    "        return default_features, default_diagrams\n",
    "\n",
    "    # Normalize point cloud\n",
    "    cloud_centered = point_cloud - np.mean(point_cloud, axis=0)\n",
    "    scale = np.std(cloud_centered)\n",
    "\n",
    "    if scale < 1e-10:  # All points essentially identical\n",
    "        return default_features, default_diagrams\n",
    "\n",
    "    cloud_normalized = cloud_centered / scale\n",
    "\n",
    "    # Compute persistent homology\n",
    "    if max_edge_length is None:\n",
    "        distances = np.linalg.norm(cloud_normalized[None, :, :] - cloud_normalized[:, None, :], axis=2)\n",
    "        distances_flat = distances[distances > 1e-10]  # Exclude near-zero\n",
    "\n",
    "        if len(distances_flat) == 0:\n",
    "            return default_features, default_diagrams\n",
    "\n",
    "        max_edge_length = np.percentile(distances_flat, 90)\n",
    "        max_edge_length = np.clip(max_edge_length, 0.1, 10.0)  # Reasonable bounds\n",
    "\n",
    "    try:\n",
    "        result = ripser(cloud_normalized, maxdim=max_dimension, thresh=max_edge_length)\n",
    "        diagrams = result['dgms']\n",
    "\n",
    "        # Ensure H1 diagram is 2D (persim requirement)\n",
    "        if max_dimension >= 1:\n",
    "            if len(diagrams) < 2 or diagrams[1].ndim == 1 or len(diagrams[1]) == 0:\n",
    "                diagrams = [diagrams[0], np.empty((0, 2))]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Ripser failed: {e}\")\n",
    "        return default_features, default_diagrams\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # H0 features\n",
    "    if len(diagrams[0]) > 0:\n",
    "        h0_persistence = diagrams[0][:, 1] - diagrams[0][:, 0]\n",
    "        finite_mask = np.isfinite(diagrams[0][:, 1])\n",
    "\n",
    "        if np.sum(finite_mask) > 0:\n",
    "            h0_pers_finite = h0_persistence[finite_mask]\n",
    "            features['h0_mean_persistence'] = np.mean(h0_pers_finite)\n",
    "            features['h0_max_persistence'] = np.max(h0_pers_finite)\n",
    "            features['h0_num_components'] = len(h0_pers_finite) + 1\n",
    "        else:\n",
    "            features['h0_mean_persistence'] = 0.0\n",
    "            features['h0_max_persistence'] = 0.0\n",
    "            features['h0_num_components'] = 1\n",
    "    else:\n",
    "        features['h0_mean_persistence'] = 0.0\n",
    "        features['h0_max_persistence'] = 0.0\n",
    "        features['h0_num_components'] = 1\n",
    "\n",
    "    # H1 features (loops/holes)\n",
    "    if max_dimension >= 1 and len(diagrams) > 1 and len(diagrams[1]) > 0:\n",
    "        h1_persistence = diagrams[1][:, 1] - diagrams[1][:, 0]\n",
    "        finite_mask = np.isfinite(diagrams[1][:, 1])\n",
    "\n",
    "        if np.sum(finite_mask) > 0:\n",
    "            h1_pers_finite = h1_persistence[finite_mask]\n",
    "            features['h1_num_holes'] = len(h1_pers_finite)\n",
    "            features['h1_max_persistence'] = np.max(h1_pers_finite)\n",
    "            features['h1_mean_persistence'] = np.mean(h1_pers_finite)\n",
    "            features['h1_sum_persistence'] = np.sum(h1_pers_finite)\n",
    "\n",
    "            h1_probs = h1_pers_finite / np.sum(h1_pers_finite)\n",
    "            features['h1_entropy'] = -np.sum(h1_probs * np.log(h1_probs + 1e-10))\n",
    "        else:\n",
    "            features['h1_num_holes'] = 0\n",
    "            features['h1_max_persistence'] = 0.0\n",
    "            features['h1_mean_persistence'] = 0.0\n",
    "            features['h1_sum_persistence'] = 0.0\n",
    "            features['h1_entropy'] = 0.0\n",
    "    else:\n",
    "        features['h1_num_holes'] = 0\n",
    "        features['h1_max_persistence'] = 0.0\n",
    "        features['h1_mean_persistence'] = 0.0\n",
    "        features['h1_sum_persistence'] = 0.0\n",
    "        features['h1_entropy'] = 0.0\n",
    "\n",
    "    return features, diagrams\n",
    "\n",
    "\n",
    "def extract_trajectory_features_with_ph(traj_fwd, traj_bwd, poincare_plane='xy'):\n",
    "    \"\"\"\n",
    "    Extract 27 features: 19 geometric + 8 topological (PH).\n",
    "    \"\"\"\n",
    "    full_traj = np.vstack([traj_bwd[::-1], traj_fwd])\n",
    "    features = []\n",
    "\n",
    "    # Spatial extent (9 features)\n",
    "    for coord in range(3):\n",
    "        features.extend([np.min(full_traj[:, coord]),\n",
    "                        np.max(full_traj[:, coord]),\n",
    "                        np.mean(full_traj[:, coord])])\n",
    "\n",
    "    # Radial features (3 features)\n",
    "    r = np.linalg.norm(full_traj, axis=1)\n",
    "    features.extend([np.min(r), np.max(r), np.mean(r)])\n",
    "\n",
    "    # Shape features (3 features)\n",
    "    features.extend([np.std(full_traj[:, i]) for i in range(3)])\n",
    "\n",
    "    # Asymmetry, arc length, tail fraction, high latitude (4 features)\n",
    "    features.append(np.abs(np.mean(full_traj[:, 2])))\n",
    "    diffs = np.diff(full_traj, axis=0)\n",
    "    features.append(np.sum(np.linalg.norm(diffs, axis=1)))\n",
    "    features.append(np.sum(full_traj[:, 0] < 0) / len(full_traj))\n",
    "    features.append(np.sum(np.abs(full_traj[:, 2]) > RM) / len(full_traj))\n",
    "\n",
    "    # Persistent homology features (8 features)\n",
    "    try:\n",
    "        poincare_points = compute_poincare_section(traj_fwd, traj_bwd, plane=poincare_plane)\n",
    "\n",
    "        if len(poincare_points) >= 4:\n",
    "            ph_features, _ = compute_persistent_homology_features(poincare_points, max_dimension=1)\n",
    "            features.extend([\n",
    "                ph_features['h0_mean_persistence'],\n",
    "                ph_features['h0_max_persistence'],\n",
    "                ph_features['h0_num_components'],\n",
    "                ph_features['h1_num_holes'],\n",
    "                ph_features['h1_max_persistence'],\n",
    "                ph_features['h1_mean_persistence'],\n",
    "                ph_features['h1_sum_persistence'],\n",
    "                ph_features['h1_entropy']\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0.0] * 8)\n",
    "    except Exception:\n",
    "        features.extend([0.0] * 8)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# OPTIMAL CLUSTER SELECTION\n",
    "# --------------------------\n",
    "def find_optimal_eps_dbscan(features_scaled, min_samples=5):\n",
    "    \"\"\"\n",
    "    Find optimal epsilon for DBSCAN using k-distance plot (elbow method).\n",
    "    \"\"\"\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "    neighbors_fit = neighbors.fit(features_scaled)\n",
    "    distances, indices = neighbors_fit.kneighbors(features_scaled)\n",
    "\n",
    "    # Sort distances\n",
    "    distances = np.sort(distances[:, -1], axis=0)\n",
    "\n",
    "    # Find elbow using maximum curvature\n",
    "    x = np.arange(len(distances))\n",
    "    y = distances\n",
    "\n",
    "    # Normalize\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "    y_norm = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "    # Find point with maximum distance to line connecting first and last points\n",
    "    # Convert to 3D vectors to avoid NumPy 2.0 deprecation warning\n",
    "    p1_3d = np.array([x_norm[0], y_norm[0], 0.0])\n",
    "    p2_3d = np.array([x_norm[-1], y_norm[-1], 0.0])\n",
    "\n",
    "    max_dist = 0\n",
    "    elbow_idx = 0\n",
    "    for i in range(len(x_norm)):\n",
    "        p_3d = np.array([x_norm[i], y_norm[i], 0.0])\n",
    "        # Cross product of 3D vectors\n",
    "        cross = np.cross(p2_3d - p1_3d, p1_3d - p_3d)\n",
    "        dist = np.linalg.norm(cross) / np.linalg.norm(p2_3d - p1_3d)\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            elbow_idx = i\n",
    "\n",
    "    optimal_eps = distances[elbow_idx]\n",
    "\n",
    "    # Optional: plot k-distance graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.axhline(y=optimal_eps, color='r', linestyle='--', label=f'Optimal eps = {optimal_eps:.3f}')\n",
    "    plt.xlabel('Data Points sorted by distance')\n",
    "    plt.ylabel(f'{min_samples}-NN Distance')\n",
    "    plt.title('K-distance Graph for Optimal Epsilon Selection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    return optimal_eps, plt\n",
    "\n",
    "\n",
    "def evaluate_clustering(features_scaled, labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering quality using multiple metrics.\n",
    "    Returns dict with scores.\n",
    "    \"\"\"\n",
    "    # Filter out noise points (label -1) for DBSCAN\n",
    "    mask = labels != -1\n",
    "    if np.sum(mask) < 2 or len(np.unique(labels[mask])) < 2:\n",
    "        return None\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # Silhouette Score: [-1, 1], higher is better\n",
    "    scores['silhouette'] = silhouette_score(features_scaled[mask], labels[mask])\n",
    "\n",
    "    # Calinski-Harabasz Index (Variance Ratio): higher is better\n",
    "    scores['calinski_harabasz'] = calinski_harabasz_score(features_scaled[mask], labels[mask])\n",
    "\n",
    "    # Davies-Bouldin Index: lower is better\n",
    "    scores['davies_bouldin'] = davies_bouldin_score(features_scaled[mask], labels[mask])\n",
    "\n",
    "    scores['n_clusters'] = len(np.unique(labels[mask]))\n",
    "    scores['n_noise'] = np.sum(labels == -1)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def find_optimal_n_clusters_hierarchical(features_scaled, max_k=10):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters for hierarchical clustering.\n",
    "    Tests multiple metrics across different k values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for k in range(2, max_k + 1):\n",
    "        clustering = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "        labels = clustering.fit_predict(features_scaled)\n",
    "\n",
    "        scores = evaluate_clustering(features_scaled, labels)\n",
    "        if scores is not None:\n",
    "            scores['k'] = k\n",
    "            results.append(scores)\n",
    "\n",
    "    # Create results dataframe\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Normalize scores for comparison (0-1 scale)\n",
    "    df_results['silhouette_norm'] = (df_results['silhouette'] - df_results['silhouette'].min()) / \\\n",
    "                                     (df_results['silhouette'].max() - df_results['silhouette'].min())\n",
    "    df_results['ch_norm'] = (df_results['calinski_harabasz'] - df_results['calinski_harabasz'].min()) / \\\n",
    "                             (df_results['calinski_harabasz'].max() - df_results['calinski_harabasz'].min())\n",
    "    # DB index: invert since lower is better\n",
    "    df_results['db_norm'] = 1 - (df_results['davies_bouldin'] - df_results['davies_bouldin'].min()) / \\\n",
    "                                 (df_results['davies_bouldin'].max() - df_results['davies_bouldin'].min())\n",
    "\n",
    "    # Combined score (equal weighting)\n",
    "    df_results['combined_score'] = (df_results['silhouette_norm'] +\n",
    "                                     df_results['ch_norm'] +\n",
    "                                     df_results['db_norm']) / 3\n",
    "\n",
    "    optimal_k = df_results.loc[df_results['combined_score'].idxmax(), 'k']\n",
    "\n",
    "    # Plot validation metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    axes[0, 0].plot(df_results['k'], df_results['silhouette'], 'o-')\n",
    "    axes[0, 0].axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Number of Clusters')\n",
    "    axes[0, 0].set_ylabel('Silhouette Score')\n",
    "    axes[0, 0].set_title('Silhouette Score (higher is better)')\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].plot(df_results['k'], df_results['calinski_harabasz'], 'o-', color='green')\n",
    "    axes[0, 1].axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Number of Clusters')\n",
    "    axes[0, 1].set_ylabel('Calinski-Harabasz Index')\n",
    "    axes[0, 1].set_title('Calinski-Harabasz Index (higher is better)')\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    axes[1, 0].plot(df_results['k'], df_results['davies_bouldin'], 'o-', color='orange')\n",
    "    axes[1, 0].axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Number of Clusters')\n",
    "    axes[1, 0].set_ylabel('Davies-Bouldin Index')\n",
    "    axes[1, 0].set_title('Davies-Bouldin Index (lower is better)')\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    axes[1, 1].plot(df_results['k'], df_results['combined_score'], 'o-', color='purple')\n",
    "    axes[1, 1].axvline(x=optimal_k, color='r', linestyle='--', alpha=0.7,\n",
    "                       label=f'Optimal k = {int(optimal_k)}')\n",
    "    axes[1, 1].set_xlabel('Number of Clusters')\n",
    "    axes[1, 1].set_ylabel('Combined Score')\n",
    "    axes[1, 1].set_title('Combined Validation Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return int(optimal_k), df_results, plt\n",
    "\n",
    "\n",
    "def cluster_trajectories_optimal(trajectory_pairs, method=\"hierarchical\",\n",
    "                                  max_k=10, min_samples=5):\n",
    "    \"\"\"\n",
    "    Cluster trajectories with automatic optimal parameter selection.\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    features = np.array([extract_trajectory_features(fwd, bwd)\n",
    "                         for fwd, bwd in trajectory_pairs])\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    if method == \"dbscan\":\n",
    "        # Find optimal epsilon\n",
    "        optimal_eps, kdist_plot = find_optimal_eps_dbscan(features_scaled, min_samples)\n",
    "\n",
    "        # Cluster with optimal parameters\n",
    "        clustering = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
    "        labels = clustering.fit_predict(features_scaled)\n",
    "\n",
    "        scores = evaluate_clustering(features_scaled, labels)\n",
    "\n",
    "        return labels, scores, kdist_plot\n",
    "\n",
    "    elif method == \"hierarchical\":\n",
    "        # Find optimal number of clusters\n",
    "        optimal_k, results_df, validation_plot = find_optimal_n_clusters_hierarchical(\n",
    "            features_scaled, max_k)\n",
    "\n",
    "        # Cluster with optimal k\n",
    "        clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "        labels = clustering.fit_predict(features_scaled)\n",
    "\n",
    "        scores = evaluate_clustering(features_scaled, labels)\n",
    "\n",
    "        return labels, scores, validation_plot, results_df\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown clustering method: {method}\")\n",
    "\n",
    "\n",
    "# function to lightly smooth field lines\n",
    "def smooth_traj(traj, k=5, order=2):\n",
    "    if traj.shape[0] < k:\n",
    "        return traj\n",
    "    return np.column_stack([\n",
    "        savgol_filter(traj[:, i], k, order, mode=\"interp\")\n",
    "        for i in range(3)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, Jx, Jy, Jz = load_field(ncfile)\n",
    "\n",
    "start = datetime.now()\n",
    "print(f\"Loaded {ncfile} at {str(start)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# TRACE FIELD LINES\n",
    "# --------------------------\n",
    "lines_by_topo = {\"closed\": [], \"open\": []}\n",
    "trajectory_pairs = {\"closed\": [], \"open\": []}\n",
    "\n",
    "for seed in seeds:\n",
    "    traj_fwd, exit_fwd_y = trace_field_line_rk(seed, Jx, Jy, Jz, x, y, z, plot_depth,\n",
    "                                                max_steps=max_steps, h=h_step)\n",
    "    traj_bwd, exit_bwd_y = trace_field_line_rk(seed, Jx, Jy, Jz, x, y, z, plot_depth,\n",
    "                                                max_steps=max_steps, h=-h_step)\n",
    "    topo = classify(traj_fwd, traj_bwd, plot_depth + surface_tol, exit_fwd_y, exit_bwd_y)\n",
    "\n",
    "    if topo in [\"closed\", \"open\"]:\n",
    "        lines_by_topo[topo].append(traj_fwd)\n",
    "        lines_by_topo[topo].append(traj_bwd)\n",
    "        trajectory_pairs[topo].append((traj_fwd, traj_bwd))\n",
    "\n",
    "classtime = datetime.now()\n",
    "print(f\"Classified all lines at {str(classtime)}\")\n",
    "print(f\"Found {len(trajectory_pairs['closed'])} closed field lines\")\n",
    "print(f\"Found {len(trajectory_pairs['open'])} open field lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLUSTER OPEN VS CLOSED TRAJECTORIES WITH OPTIMAL PARAMETERS\n",
    "# --------------------------\n",
    "cluster_labels = {\"closed\": None, \"open\": None}\n",
    "lines_by_cluster = {}\n",
    "all_results = {}\n",
    "all_scores = {}\n",
    "\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if len(trajectory_pairs[topo]) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Clustering {topo} field lines...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if clustering_method == \"hierarchical\":\n",
    "        labels, scores, validation_plot, results_df = cluster_trajectories_optimal(\n",
    "            trajectory_pairs[topo],\n",
    "            method=clustering_method,\n",
    "            max_k=max_clusters_to_test\n",
    "        )\n",
    "\n",
    "        # Save validation plots\n",
    "        validation_plot.savefig(\n",
    "            os.path.join(output_folder, f\"{case}_{step}_{topo}_validation_metrics.png\"),\n",
    "            dpi=150, bbox_inches='tight'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Save results table\n",
    "        results_df.to_csv(\n",
    "            os.path.join(output_folder, f\"{case}_{step}_{topo}_clustering_scores.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        all_results[topo] = results_df\n",
    "        all_scores[topo] = scores\n",
    "\n",
    "    else:  # DBSCAN\n",
    "        labels, scores, kdist_plot = cluster_trajectories_optimal(\n",
    "            trajectory_pairs[topo],\n",
    "            method=clustering_method\n",
    "        )\n",
    "\n",
    "        # Save k-distance plot\n",
    "        kdist_plot.savefig(\n",
    "            os.path.join(output_folder, f\"{case}_{step}_{topo}_kdistance_plot.png\"),\n",
    "            dpi=150, bbox_inches='tight'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        all_scores[topo] = scores\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nOptimal clustering results:\")\n",
    "    print(f\"  Number of clusters: {scores['n_clusters']}\")\n",
    "    if 'n_noise' in scores:\n",
    "        n_noise = scores['n_noise']\n",
    "        n_total = len(trajectory_pairs[topo])\n",
    "        pct_noise = 100 * n_noise / n_total\n",
    "        print(f\"  Noise points: {n_noise} ({pct_noise:.1f}%)\")\n",
    "    print(f\"  Silhouette Score: {scores['silhouette']:.3f}\")\n",
    "    print(f\"  Calinski-Harabasz Index: {scores['calinski_harabasz']:.1f}\")\n",
    "    print(f\"  Davies-Bouldin Index: {scores['davies_bouldin']:.3f}\")\n",
    "\n",
    "    cluster_labels[topo] = labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = []\n",
    "    for cluster_id in unique_labels:\n",
    "        if cluster_id == -1:  # Skip noise for DBSCAN\n",
    "            continue\n",
    "        mask = labels == cluster_id\n",
    "        count = np.sum(mask)\n",
    "        cluster_sizes.append((cluster_id, count))\n",
    "\n",
    "    # Sort by size for better readability\n",
    "    cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n  Cluster sizes (sorted by count):\")\n",
    "\n",
    "    # Organize trajectories by cluster\n",
    "    for cluster_id, count in cluster_sizes:\n",
    "        cluster_key = f\"{topo}_cluster_{cluster_id}\"\n",
    "        lines_by_cluster[cluster_key] = []\n",
    "\n",
    "        mask = labels == cluster_id\n",
    "        for idx, (traj_fwd, traj_bwd) in enumerate(trajectory_pairs[topo]):\n",
    "            if mask[idx]:\n",
    "                lines_by_cluster[cluster_key].append(traj_fwd)\n",
    "                lines_by_cluster[cluster_key].append(traj_bwd)\n",
    "\n",
    "        pct = 100 * count / len(trajectory_pairs[topo])\n",
    "        print(f\"    Cluster {cluster_id}: {count:4d} field lines ({pct:5.1f}%)\")\n",
    "\n",
    "    # Add noise to dictionary if using DBSCAN\n",
    "    if clustering_method == \"dbscan\" and -1 in unique_labels:\n",
    "        noise_key = f\"{topo}_noise\"\n",
    "        lines_by_cluster[noise_key] = []\n",
    "\n",
    "        mask = labels == -1\n",
    "        for idx, (traj_fwd, traj_bwd) in enumerate(trajectory_pairs[topo]):\n",
    "            if mask[idx]:\n",
    "                lines_by_cluster[noise_key].append(traj_fwd)\n",
    "                lines_by_cluster[noise_key].append(traj_bwd)\n",
    "\n",
    "clustertime = datetime.now()\n",
    "print(f\"\\nCompleted open vs closed clustering at {str(clustertime)}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CLUSTERING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if cluster_labels[topo] is not None:\n",
    "        scores = all_scores[topo]\n",
    "        labels = cluster_labels[topo]\n",
    "        n_clusters = scores['n_clusters']\n",
    "        n_total = len(trajectory_pairs[topo])\n",
    "        n_clustered = np.sum(labels != -1)\n",
    "\n",
    "        print(f\"\\n{topo.upper()}:\")\n",
    "        print(f\"  Total field lines: {n_total}\")\n",
    "        print(f\"  Clustered: {n_clustered} ({100*n_clustered/n_total:.1f}%)\")\n",
    "        if 'n_noise' in scores:\n",
    "            print(f\"  Noise: {scores['n_noise']} ({100*scores['n_noise']/n_total:.1f}%)\")\n",
    "        print(f\"  Number of clusters: {n_clusters}\")\n",
    "        print(f\"  Quality metrics:\")\n",
    "        print(f\"    Silhouette: {scores['silhouette']:.3f} (range: -1 to 1, higher is better)\")\n",
    "        print(f\"    Calinski-Harabasz: {scores['calinski_harabasz']:.1f} (higher is better)\")\n",
    "        print(f\"    Davies-Bouldin: {scores['davies_bouldin']:.3f} (lower is better)\")\n",
    "\n",
    "        # Check for highly imbalanced clusters\n",
    "        cluster_counts = []\n",
    "        for cluster_id in np.unique(labels):\n",
    "            if cluster_id != -1:\n",
    "                cluster_counts.append(np.sum(labels == cluster_id))\n",
    "\n",
    "        if len(cluster_counts) > 0:\n",
    "            largest = max(cluster_counts)\n",
    "            smallest = min(cluster_counts)\n",
    "            imbalance_ratio = largest / smallest if smallest > 0 else float('inf')\n",
    "\n",
    "            if imbalance_ratio > 100:\n",
    "                print(f\"  ⚠️  WARNING: Highly imbalanced clusters detected!\")\n",
    "                print(f\"      Largest cluster: {largest} | Smallest cluster: {smallest}\")\n",
    "                print(f\"      Consider adjusting clustering parameters or trying hierarchical method.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PLOT 3D FIELD LINES WITH SEPARATE LEGENDS\n",
    "# --------------------------\n",
    "def generate_colors(n):\n",
    "    \"\"\"Generate visually distinct colors using tab10-style palette\"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Tab10 colors (10 maximally distinct colors)\n",
    "    tab10_colors = [\n",
    "        '#1f77b4',  # Blue\n",
    "        '#ff7f0e',  # Orange\n",
    "        '#2ca02c',  # Green\n",
    "        '#d62728',  # Red\n",
    "        '#9467bd',  # Purple\n",
    "        '#8c564b',  # Brown\n",
    "        '#e377c2',  # Pink\n",
    "        '#7f7f7f',  # Gray\n",
    "        '#bcbd22',  # Olive\n",
    "        '#17becf',  # Cyan\n",
    "    ]\n",
    "\n",
    "    # If we need more than 10 colors, cycle through with slight variations\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        base_color = tab10_colors[i % 10]\n",
    "\n",
    "        if i < 10:\n",
    "            colors.append(base_color)\n",
    "        else:\n",
    "            # For additional colors, darken or lighten the base colors\n",
    "            rgb = mcolors.hex2color(base_color)\n",
    "            factor = 0.7 if (i // 10) % 2 == 0 else 1.3\n",
    "            rgb_adjusted = tuple(min(1.0, max(0.0, c * factor)) for c in rgb)\n",
    "            colors.append(f'rgb({int(rgb_adjusted[0]*255)},{int(rgb_adjusted[1]*255)},{int(rgb_adjusted[2]*255)})')\n",
    "\n",
    "    return colors\n",
    "\n",
    "\n",
    "# Separate clusters by topology\n",
    "closed_clusters = sorted([k for k in lines_by_cluster.keys() if k.startswith('closed')],\n",
    "                        key=lambda x: int(x.split('_')[-1]))\n",
    "open_clusters = sorted([k for k in lines_by_cluster.keys() if k.startswith('open')],\n",
    "                      key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "# Generate colors for each topology separately\n",
    "closed_colors = generate_colors(len(closed_clusters))\n",
    "open_colors = generate_colors(len(open_clusters))\n",
    "\n",
    "colors_dict = {}\n",
    "for key, color in zip(closed_clusters, closed_colors):\n",
    "    colors_dict[key] = color\n",
    "for key, color in zip(open_clusters, open_colors):\n",
    "    colors_dict[key] = color\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_stats = {}\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if cluster_labels[topo] is None:\n",
    "        continue\n",
    "    labels = cluster_labels[topo]\n",
    "    unique_labels = np.unique(labels[labels != -1])\n",
    "\n",
    "    for cluster_id in unique_labels:\n",
    "        cluster_key = f\"{topo}_cluster_{cluster_id}\"\n",
    "        n_lines = np.sum(labels == cluster_id)\n",
    "        cluster_stats[cluster_key] = n_lines\n",
    "\n",
    "# Prepare title statistics\n",
    "title_stats = []\n",
    "if cluster_labels[\"closed\"] is not None:\n",
    "    closed_scores = all_results.get(\"closed\")\n",
    "    if closed_scores is not None:\n",
    "        closed_sil = closed_scores.loc[closed_scores['combined_score'].idxmax(), 'silhouette']\n",
    "        closed_ch = closed_scores.loc[closed_scores['combined_score'].idxmax(), 'calinski_harabasz']\n",
    "    else:\n",
    "        closed_sil = closed_ch = 0\n",
    "    title_stats.append(f\"Closed: {len(closed_clusters)} clusters (S={closed_sil:.2f}, CH={closed_ch:.0f})\")\n",
    "\n",
    "if cluster_labels[\"open\"] is not None:\n",
    "    open_scores = all_results.get(\"open\")\n",
    "    if open_scores is not None:\n",
    "        open_sil = open_scores.loc[open_scores['combined_score'].idxmax(), 'silhouette']\n",
    "        open_ch = open_scores.loc[open_scores['combined_score'].idxmax(), 'calinski_harabasz']\n",
    "    else:\n",
    "        open_sil = open_ch = 0\n",
    "    title_stats.append(f\"Open: {len(open_clusters)} clusters (S={open_sil:.2f}, CH={open_ch:.0f})\")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add planet sphere\n",
    "theta = np.linspace(0, np.pi, 100)\n",
    "phi = np.linspace(0, 2*np.pi, 200)\n",
    "theta, phi = np.meshgrid(theta, phi)\n",
    "\n",
    "xs = plot_depth * np.sin(theta) * np.cos(phi)\n",
    "ys = plot_depth * np.sin(theta) * np.sin(phi)\n",
    "zs = plot_depth * np.cos(theta)\n",
    "\n",
    "eps = 0\n",
    "mask_pos = xs >= -eps\n",
    "mask_neg = xs <= eps\n",
    "\n",
    "# Dayside hemisphere\n",
    "fig.add_trace(go.Surface(\n",
    "    x=np.where(mask_pos, xs, np.nan),\n",
    "    y=np.where(mask_pos, ys, np.nan),\n",
    "    z=np.where(mask_pos, zs, np.nan),\n",
    "    surfacecolor=np.ones_like(xs),\n",
    "    colorscale=[[0, 'lightgrey'], [1, 'lightgrey']],\n",
    "    cmin=0, cmax=1,\n",
    "    showscale=False,\n",
    "    lighting=dict(ambient=1, diffuse=0, specular=0),\n",
    "    hoverinfo='skip',\n",
    "    legendgroup='planet',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Nightside hemisphere\n",
    "fig.add_trace(go.Surface(\n",
    "    x=np.where(mask_neg, xs, np.nan),\n",
    "    y=np.where(mask_neg, ys, np.nan),\n",
    "    z=np.where(mask_neg, zs, np.nan),\n",
    "    surfacecolor=np.zeros_like(xs),\n",
    "    colorscale=[[0, 'black'], [1, 'black']],\n",
    "    cmin=0, cmax=1,\n",
    "    showscale=False,\n",
    "    lighting=dict(ambient=1, diffuse=0, specular=0),\n",
    "    hoverinfo='skip',\n",
    "    legendgroup='planet',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Add closed field lines\n",
    "print(\"\\nPlotting closed field lines...\")\n",
    "for cluster_key in closed_clusters:\n",
    "    lines = lines_by_cluster[cluster_key]\n",
    "    if len(lines) == 0:\n",
    "        continue\n",
    "\n",
    "    cluster_id = int(cluster_key.split('_')[-1])\n",
    "    n_lines = cluster_stats[cluster_key]\n",
    "    legend_label = f\"Closed {cluster_id} (n={n_lines})\"\n",
    "\n",
    "    first = True\n",
    "\n",
    "    if len(lines) > max_lines:\n",
    "        lines_to_plot = random.sample(lines, max_lines)\n",
    "    else:\n",
    "        lines_to_plot = lines\n",
    "\n",
    "    for traj in lines_to_plot:\n",
    "        traj_s = smooth_traj(traj)\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=traj_s[:, 0],\n",
    "            y=traj_s[:, 1],\n",
    "            z=traj_s[:, 2],\n",
    "            mode='lines',\n",
    "            line=dict(color=colors_dict[cluster_key], width=2),\n",
    "            name=legend_label,\n",
    "            legendgroup=cluster_key,\n",
    "            legendgrouptitle_text=\"Closed\" if cluster_key == closed_clusters[0] else None,\n",
    "            showlegend=first,\n",
    "            hovertemplate=(\n",
    "                f'<b>{legend_label}</b><br>' +\n",
    "                'X: %{x:.0f} km<br>' +\n",
    "                'Y: %{y:.0f} km<br>' +\n",
    "                'Z: %{z:.0f} km<br>' +\n",
    "                '<extra></extra>'\n",
    "            )\n",
    "        ))\n",
    "        first = False\n",
    "\n",
    "    print(f\"  {legend_label}: plotted {len(lines_to_plot)} line segments\")\n",
    "\n",
    "# Add open field lines\n",
    "print(\"\\nPlotting open field lines...\")\n",
    "for cluster_key in open_clusters:\n",
    "    lines = lines_by_cluster[cluster_key]\n",
    "    if len(lines) == 0:\n",
    "        continue\n",
    "\n",
    "    cluster_id = int(cluster_key.split('_')[-1])\n",
    "    n_lines = cluster_stats[cluster_key]\n",
    "    legend_label = f\"Open {cluster_id} (n={n_lines})\"\n",
    "\n",
    "    first = True\n",
    "\n",
    "    if len(lines) > max_lines:\n",
    "        lines_to_plot = random.sample(lines, max_lines)\n",
    "    else:\n",
    "        lines_to_plot = lines\n",
    "\n",
    "    for traj in lines_to_plot:\n",
    "        traj_s = smooth_traj(traj)\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=traj_s[:, 0],\n",
    "            y=traj_s[:, 1],\n",
    "            z=traj_s[:, 2],\n",
    "            mode='lines',\n",
    "            line=dict(color=colors_dict[cluster_key], width=2),\n",
    "            name=legend_label,\n",
    "            legendgroup=cluster_key,\n",
    "            legendgrouptitle_text=\"Open\" if cluster_key == open_clusters[0] else None,\n",
    "            showlegend=first,\n",
    "            hovertemplate=(\n",
    "                f'<b>{legend_label}</b><br>' +\n",
    "                'X: %{x:.0f} km<br>' +\n",
    "                'Y: %{y:.0f} km<br>' +\n",
    "                'Z: %{z:.0f} km<br>' +\n",
    "                '<extra></extra>'\n",
    "            )\n",
    "        ))\n",
    "        first = False\n",
    "\n",
    "    print(f\"  {legend_label}: plotted {len(lines_to_plot)} line segments\")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1400,\n",
    "    height=1000,\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='X [km]',\n",
    "            range=[-12 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Y [km]',\n",
    "            range=[-4.5 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Z [km]',\n",
    "            range=[-4.5 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=2.9, y=1.8, z=1.8),\n",
    "        camera=dict(\n",
    "            eye=dict(x=1.5, y=1.5, z=1.2),\n",
    "            center=dict(x=0, y=0, z=0)\n",
    "        )\n",
    "    ),\n",
    "    legend=dict(\n",
    "        groupclick=\"togglegroup\",\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=10),\n",
    "        itemsizing='constant',\n",
    "        x=1.02,\n",
    "        y=0.5,\n",
    "        xanchor='left',\n",
    "        yanchor='middle',\n",
    "        tracegroupgap=10\n",
    "    ),\n",
    "    title=dict(\n",
    "        text=(\n",
    "            f\"<b>{case} Current Field Clustered Topology</b><br>\"\n",
    "            f\"<sup>t = {step*0.002} s | {' | '.join(title_stats)}</sup>\"\n",
    "        ),\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    margin=dict(l=0, r=200, t=100, b=0)\n",
    ")\n",
    "\n",
    "# Save outputs\n",
    "out_html = f\"{case}_{step}_J_openvclosed_clustered_topology_optimal.html\"\n",
    "out_png = out_html.replace(\".html\", \".png\")\n",
    "\n",
    "fig.write_html(os.path.join(output_folder, out_html), include_plotlyjs=\"cdn\")\n",
    "fig.write_image(os.path.join(output_folder, out_png), scale=2)\n",
    "\n",
    "plottime = datetime.now()\n",
    "print(f\"\\nSaved open vs closed figure at {str(plottime)}\")\n",
    "print(f\"  HTML: {out_html}\")\n",
    "print(f\"  PNG:  {out_png}\")\n",
    "\n",
    "# Create summary statistics file\n",
    "summary_stats = []\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if cluster_labels[topo] is None:\n",
    "        continue\n",
    "\n",
    "    labels = cluster_labels[topo]\n",
    "    unique_labels = np.unique(labels[labels != -1])\n",
    "\n",
    "    for cluster_id in unique_labels:\n",
    "        cluster_key = f\"{topo}_cluster_{cluster_id}\"\n",
    "        n_lines = cluster_stats[cluster_key]\n",
    "        summary_stats.append({\n",
    "            'topology': topo,\n",
    "            'cluster_id': cluster_id,\n",
    "            'n_field_lines': n_lines\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "summary_file = f\"{case}_{step}_openvclosed_cluster_summary.csv\"\n",
    "df_summary.to_csv(os.path.join(output_folder, summary_file), index=False)\n",
    "print(f\"  Summary: {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLUSTER ALL TRAJECTORIES TOGETHER WITH OPTIMAL PARAMETERS\n",
    "# --------------------------\n",
    "lines_by_cluster_all = {}\n",
    "\n",
    "# Combine all trajectory pairs regardless of topology\n",
    "all_trajectory_pairs = []\n",
    "all_topology_labels = []  # Keep track of original topology for reference\n",
    "\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    for traj_pair in trajectory_pairs[topo]:\n",
    "        all_trajectory_pairs.append(traj_pair)\n",
    "        all_topology_labels.append(topo)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Clustering all {len(all_trajectory_pairs)} field lines together...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if clustering_method == \"hierarchical\":\n",
    "    all_labels, all_scores, all_validation_plot, all_results_df = cluster_trajectories_optimal(\n",
    "        all_trajectory_pairs,\n",
    "        method=clustering_method,\n",
    "        max_k=max_clusters_to_test\n",
    "    )\n",
    "\n",
    "    # Save validation plots\n",
    "    all_validation_plot.savefig(\n",
    "        os.path.join(output_folder, f\"{case}_{step}_all_validation_metrics.png\"),\n",
    "        dpi=150, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Save results table\n",
    "    all_results_df.to_csv(\n",
    "        os.path.join(output_folder, f\"{case}_{step}_all_clustering_scores.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "else:  # DBSCAN\n",
    "    all_labels, all_scores, all_kdist_plot = cluster_trajectories_optimal(\n",
    "        all_trajectory_pairs,\n",
    "        method=clustering_method\n",
    "    )\n",
    "\n",
    "    # Save k-distance plot\n",
    "    all_kdist_plot.savefig(\n",
    "        os.path.join(output_folder, f\"{case}_{step}_all_kdistance_plot.png\"),\n",
    "        dpi=150, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nOptimal clustering results:\")\n",
    "print(f\"  Number of clusters: {all_scores['n_clusters']}\")\n",
    "if 'n_noise' in all_scores:\n",
    "    n_noise = all_scores['n_noise']\n",
    "    n_total = len(all_trajectory_pairs)\n",
    "    pct_noise = 100 * n_noise / n_total\n",
    "    print(f\"  Noise points: {n_noise} ({pct_noise:.1f}%)\")\n",
    "print(f\"  Silhouette Score: {all_scores['silhouette']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {all_scores['calinski_harabasz']:.1f}\")\n",
    "print(f\"  Davies-Bouldin Index: {all_scores['davies_bouldin']:.3f}\")\n",
    "\n",
    "all_unique_labels = np.unique(all_labels)\n",
    "\n",
    "# Analyze topology composition of each cluster\n",
    "print(f\"\\nCluster composition:\")\n",
    "cluster_composition = []\n",
    "for cluster_id in all_unique_labels:\n",
    "    if cluster_id == -1:  # Skip noise for DBSCAN\n",
    "        continue\n",
    "\n",
    "    mask = all_labels == cluster_id\n",
    "    cluster_topos = [all_topology_labels[i] for i in range(len(all_topology_labels)) if mask[i]]\n",
    "    n_closed = cluster_topos.count(\"closed\")\n",
    "    n_open = cluster_topos.count(\"open\")\n",
    "    total = n_closed + n_open\n",
    "\n",
    "    cluster_composition.append((cluster_id, total, n_closed, n_open))\n",
    "    print(f\"  Cluster {cluster_id}: {total} field lines \"\n",
    "          f\"({n_closed} closed, {n_open} open)\")\n",
    "\n",
    "# Sort by total size for better organization\n",
    "cluster_composition.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Organize trajectories by cluster\n",
    "for cluster_id, total, n_closed, n_open in cluster_composition:\n",
    "    cluster_key = f\"all_cluster_{cluster_id}\"\n",
    "    lines_by_cluster_all[cluster_key] = []\n",
    "\n",
    "    mask = all_labels == cluster_id\n",
    "    for idx, (traj_fwd, traj_bwd) in enumerate(all_trajectory_pairs):\n",
    "        if mask[idx]:\n",
    "            lines_by_cluster_all[cluster_key].append(traj_fwd)\n",
    "            lines_by_cluster_all[cluster_key].append(traj_bwd)\n",
    "\n",
    "clustertime = datetime.now()\n",
    "print(f\"\\nCompleted all-trajectory clustering at {str(clustertime)}\")\n",
    "\n",
    "# Save cluster assignments with topology information\n",
    "all_cluster_data = []\n",
    "for idx, label in enumerate(all_labels):\n",
    "    all_cluster_data.append({\n",
    "        'trajectory_idx': idx,\n",
    "        'cluster_id': label,\n",
    "        'original_topology': all_topology_labels[idx]\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(all_cluster_data)\n",
    "df_all.to_csv(os.path.join(output_folder, f\"{case}_{step}_alltraj_cluster_assignments.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PLOT 3D FIELD LINES BY CLUSTER (ALL TRAJECTORIES)\n",
    "# --------------------------\n",
    "def generate_colors(n):\n",
    "    \"\"\"\n",
    "    Generate visually distinct colors using tab10-style palette\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Tab10 colors (10 maximally distinct colors)\n",
    "    tab10_colors = [\n",
    "        '#1f77b4',  # Blue\n",
    "        '#ff7f0e',  # Orange\n",
    "        '#2ca02c',  # Green\n",
    "        '#d62728',  # Red\n",
    "        '#9467bd',  # Purple\n",
    "        '#8c564b',  # Brown\n",
    "        '#e377c2',  # Pink\n",
    "        '#7f7f7f',  # Gray\n",
    "        '#bcbd22',  # Olive\n",
    "        '#17becf',  # Cyan\n",
    "    ]\n",
    "\n",
    "    # If we need more than 10 colors, cycle through with slight variations\n",
    "    colors = []\n",
    "    for i in range(n):\n",
    "        base_color = tab10_colors[i % 10]\n",
    "\n",
    "        if i < 10:\n",
    "            colors.append(base_color)\n",
    "        else:\n",
    "            # For additional colors, darken or lighten the base colors\n",
    "            rgb = mcolors.hex2color(base_color)\n",
    "            factor = 0.7 if (i // 10) % 2 == 0 else 1.3\n",
    "            rgb_adjusted = tuple(min(1.0, max(0.0, c * factor)) for c in rgb)\n",
    "            colors.append(f'rgb({int(rgb_adjusted[0]*255)},{int(rgb_adjusted[1]*255)},{int(rgb_adjusted[2]*255)})')\n",
    "\n",
    "    return colors\n",
    "\n",
    "\n",
    "# Sort clusters by ID for consistent ordering\n",
    "sorted_all_clusters = sorted(lines_by_cluster_all.keys(),\n",
    "                             key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "all_cluster_colors = generate_colors(len(sorted_all_clusters))\n",
    "all_colors_dict = {key: color for key, color in zip(sorted_all_clusters, all_cluster_colors)}\n",
    "\n",
    "# Calculate cluster statistics for legend labels\n",
    "all_cluster_stats = {}\n",
    "for cluster_key in sorted_all_clusters:\n",
    "    cluster_id = int(cluster_key.split('_')[-1])\n",
    "    mask = all_labels == cluster_id\n",
    "    cluster_topos = [all_topology_labels[i] for i in range(len(all_topology_labels)) if mask[i]]\n",
    "    n_closed = cluster_topos.count(\"closed\")\n",
    "    n_open = cluster_topos.count(\"open\")\n",
    "    n_total = n_closed + n_open\n",
    "    all_cluster_stats[cluster_key] = {\n",
    "        'total': n_total,\n",
    "        'closed': n_closed,\n",
    "        'open': n_open\n",
    "    }\n",
    "\n",
    "fig_all = go.Figure()\n",
    "\n",
    "# Add planet sphere\n",
    "theta = np.linspace(0, np.pi, 100)\n",
    "phi = np.linspace(0, 2*np.pi, 200)\n",
    "theta, phi = np.meshgrid(theta, phi)\n",
    "\n",
    "xs = plot_depth * np.sin(theta) * np.cos(phi)\n",
    "ys = plot_depth * np.sin(theta) * np.sin(phi)\n",
    "zs = plot_depth * np.cos(theta)\n",
    "\n",
    "eps = 0\n",
    "mask_pos = xs >= -eps\n",
    "mask_neg = xs <= eps\n",
    "\n",
    "# Dayside hemisphere (light grey)\n",
    "fig_all.add_trace(go.Surface(\n",
    "    x=np.where(mask_pos, xs, np.nan),\n",
    "    y=np.where(mask_pos, ys, np.nan),\n",
    "    z=np.where(mask_pos, zs, np.nan),\n",
    "    surfacecolor=np.ones_like(xs),\n",
    "    colorscale=[[0, 'lightgrey'], [1, 'lightgrey']],\n",
    "    cmin=0, cmax=1,\n",
    "    showscale=False,\n",
    "    lighting=dict(ambient=1, diffuse=0, specular=0),\n",
    "    hoverinfo='skip',\n",
    "    name='Mercury (dayside)'\n",
    "))\n",
    "\n",
    "# Nightside hemisphere (black)\n",
    "fig_all.add_trace(go.Surface(\n",
    "    x=np.where(mask_neg, xs, np.nan),\n",
    "    y=np.where(mask_neg, ys, np.nan),\n",
    "    z=np.where(mask_neg, zs, np.nan),\n",
    "    surfacecolor=np.zeros_like(xs),\n",
    "    colorscale=[[0, 'black'], [1, 'black']],\n",
    "    cmin=0, cmax=1,\n",
    "    showscale=False,\n",
    "    lighting=dict(ambient=1, diffuse=0, specular=0),\n",
    "    hoverinfo='skip',\n",
    "    name='Mercury (nightside)'\n",
    "))\n",
    "\n",
    "# Add field lines by cluster\n",
    "print(\"\\nPlotting all-trajectory field lines...\")\n",
    "for cluster_key in sorted_all_clusters:\n",
    "    lines = lines_by_cluster_all[cluster_key]\n",
    "    if len(lines) == 0:\n",
    "        continue\n",
    "\n",
    "    stats = all_cluster_stats[cluster_key]\n",
    "    cluster_id = int(cluster_key.split('_')[-1])\n",
    "\n",
    "    # Create informative legend label\n",
    "    legend_label = (f\"Cluster {cluster_id} \"\n",
    "                   f\"(n={stats['total']}: \"\n",
    "                   f\"{stats['closed']}C/{stats['open']}O)\")\n",
    "\n",
    "    first = True\n",
    "\n",
    "    # Downsample lines if necessary\n",
    "    if len(lines) > max_lines:\n",
    "        lines_to_plot = random.sample(lines, max_lines)\n",
    "    else:\n",
    "        lines_to_plot = lines\n",
    "\n",
    "    for traj in lines_to_plot:\n",
    "        traj_s = smooth_traj(traj)\n",
    "\n",
    "        fig_all.add_trace(go.Scatter3d(\n",
    "            x=traj_s[:, 0],\n",
    "            y=traj_s[:, 1],\n",
    "            z=traj_s[:, 2],\n",
    "            mode='lines',\n",
    "            line=dict(color=all_colors_dict[cluster_key], width=2),\n",
    "            name=legend_label,\n",
    "            legendgroup=cluster_key,\n",
    "            showlegend=first,\n",
    "            hovertemplate=(\n",
    "                f'<b>{legend_label}</b><br>' +\n",
    "                'X: %{x:.0f} km<br>' +\n",
    "                'Y: %{y:.0f} km<br>' +\n",
    "                'Z: %{z:.0f} km<br>' +\n",
    "                '<extra></extra>'\n",
    "            )\n",
    "        ))\n",
    "        first = False\n",
    "\n",
    "    print(f\"  {legend_label}: plotted {len(lines_to_plot)} line segments\")\n",
    "\n",
    "# Update layout with better styling\n",
    "fig_all.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1400,\n",
    "    height=1000,\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='X [km]',\n",
    "            range=[-12 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Y [km]',\n",
    "            range=[-4.5 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Z [km]',\n",
    "            range=[-4.5 * RM, 4.5 * RM],\n",
    "            backgroundcolor=\"rgb(230, 230, 230)\",\n",
    "            gridcolor=\"white\",\n",
    "            showbackground=True\n",
    "        ),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=2.9, y=1.8, z=1.8),\n",
    "        camera=dict(\n",
    "            eye=dict(x=1.5, y=1.5, z=1.2),\n",
    "            center=dict(x=0, y=0, z=0)\n",
    "        )\n",
    "    ),\n",
    "    legend=dict(\n",
    "        groupclick=\"togglegroup\",\n",
    "        title=dict(\n",
    "            text=\"<b>Field Line Clusters</b><br>(C=Closed, O=Open)\",\n",
    "            font=dict(size=12)\n",
    "        ),\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=10),\n",
    "        itemsizing='constant',\n",
    "        x=1.02,\n",
    "        y=0.5,\n",
    "        xanchor='left',\n",
    "        yanchor='middle'\n",
    "    ),\n",
    "    title=dict(\n",
    "        text=f\"<b>{case} Current Field Clustered Topology (All Trajectories)</b><br>\" +\n",
    "             f\"<sup>t = {step*0.002} s | {len(sorted_all_clusters)} clusters | \" +\n",
    "             f\"Silhouette: {all_scores['silhouette']:.3f} | \" +\n",
    "             f\"CH Index: {all_scores['calinski_harabasz']:.1f}</sup>\",\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    margin=dict(l=0, r=200, t=80, b=0)\n",
    ")\n",
    "\n",
    "# Save outputs\n",
    "out_html_all = f\"{case}_{step}_J_alltraj_clustered_topology_optimal.html\"\n",
    "out_png_all = out_html_all.replace(\".html\", \".png\")\n",
    "\n",
    "fig_all.write_html(os.path.join(output_folder, out_html_all), include_plotlyjs=\"cdn\")\n",
    "fig_all.write_image(os.path.join(output_folder, out_png_all), scale=2)\n",
    "\n",
    "plottime_all = datetime.now()\n",
    "print(f\"\\nSaved all trajectory figure at {str(plottime_all)}\")\n",
    "print(f\"  HTML: {out_html_all}\")\n",
    "print(f\"  PNG:  {out_png_all}\")\n",
    "\n",
    "# Create summary statistics file for all-trajectory clustering\n",
    "summary_stats_all = []\n",
    "for cluster_key in sorted_all_clusters:\n",
    "    cluster_id = int(cluster_key.split('_')[-1])\n",
    "    stats = all_cluster_stats[cluster_key]\n",
    "    summary_stats_all.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'n_total': stats['total'],\n",
    "        'n_closed': stats['closed'],\n",
    "        'n_open': stats['open'],\n",
    "        'pct_closed': 100 * stats['closed'] / stats['total'],\n",
    "        'pct_open': 100 * stats['open'] / stats['total']\n",
    "    })\n",
    "\n",
    "df_summary_all = pd.DataFrame(summary_stats_all)\n",
    "summary_file_all = f\"{case}_{step}_alltraj_cluster_summary.csv\"\n",
    "df_summary_all.to_csv(os.path.join(output_folder, summary_file_all), index=False)\n",
    "print(f\"  Summary: {summary_file_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PH-ENHANCED CLUSTERING\n",
    "# ===============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PERSISTENT HOMOLOGY-ENHANCED CLUSTERING\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "cluster_labels_ph = {\"closed\": None, \"open\": None}\n",
    "lines_by_cluster_ph = {}\n",
    "all_scores_ph = {}\n",
    "all_features_ph = {}\n",
    "\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if len(trajectory_pairs[topo]) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Clustering {topo} field lines with PH features...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Extract PH-enhanced features\n",
    "    features_ph = []\n",
    "    for i, (fwd, bwd) in enumerate(trajectory_pairs[topo]):\n",
    "        feat = extract_trajectory_features_with_ph(fwd, bwd)\n",
    "        features_ph.append(feat)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(trajectory_pairs[topo])} trajectories\")\n",
    "\n",
    "    features_ph = np.array(features_ph)\n",
    "    print(f\"\\nFeature shape: {features_ph.shape} (19 geometric + 8 topological)\")\n",
    "\n",
    "    # Standardize and cluster\n",
    "    scaler_ph = StandardScaler()\n",
    "    features_scaled_ph = scaler_ph.fit_transform(features_ph)\n",
    "\n",
    "    if clustering_method == \"hierarchical\":\n",
    "        optimal_k_ph, results_df_ph, val_plot_ph = find_optimal_n_clusters_hierarchical(\n",
    "            features_scaled_ph, max_k=max_clusters_to_test\n",
    "        )\n",
    "        clustering_ph = AgglomerativeClustering(n_clusters=optimal_k_ph, linkage='ward')\n",
    "        labels_ph = clustering_ph.fit_predict(features_scaled_ph)\n",
    "        scores_ph = evaluate_clustering(features_scaled_ph, labels_ph)\n",
    "\n",
    "        # Save outputs\n",
    "        val_plot_ph.savefig(\n",
    "            os.path.join(output_folder, f\"{case}_{step}_{topo}_ph_validation.png\"),\n",
    "            dpi=150, bbox_inches='tight'\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        results_df_ph.to_csv(\n",
    "            os.path.join(output_folder, f\"{case}_{step}_{topo}_ph_scores.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # Print results and comparison\n",
    "    print(f\"\\nPH-Enhanced Results:\")\n",
    "    print(f\"  Clusters: {scores_ph['n_clusters']}\")\n",
    "    print(f\"  Silhouette: {scores_ph['silhouette']:.3f}\")\n",
    "    print(f\"  Calinski-Harabasz: {scores_ph['calinski_harabasz']:.1f}\")\n",
    "    print(f\"  Davies-Bouldin: {scores_ph['davies_bouldin']:.3f}\")\n",
    "\n",
    "    if cluster_labels.get(topo) is not None:\n",
    "        orig = all_scores[topo]\n",
    "        print(f\"\\n  vs Geometric-only:\")\n",
    "        print(f\"    Silhouette: {orig['silhouette']:.3f} → {scores_ph['silhouette']:.3f} \"\n",
    "              f\"({'+' if scores_ph['silhouette'] > orig['silhouette'] else ''}\"\n",
    "              f\"{scores_ph['silhouette'] - orig['silhouette']:.3f})\")\n",
    "        print(f\"    CH Index: {orig['calinski_harabasz']:.1f} → {scores_ph['calinski_harabasz']:.1f}\")\n",
    "        print(f\"    DB Index: {orig['davies_bouldin']:.3f} → {scores_ph['davies_bouldin']:.3f}\")\n",
    "\n",
    "    cluster_labels_ph[topo] = labels_ph\n",
    "    all_scores_ph[topo] = scores_ph\n",
    "    all_features_ph[topo] = features_ph\n",
    "\n",
    "    # Organize by cluster\n",
    "    for cluster_id in np.unique(labels_ph[labels_ph != -1]):\n",
    "        cluster_key = f\"{topo}_cluster_ph_{cluster_id}\"\n",
    "        lines_by_cluster_ph[cluster_key] = []\n",
    "        mask = labels_ph == cluster_id\n",
    "        for idx, (traj_fwd, traj_bwd) in enumerate(trajectory_pairs[topo]):\n",
    "            if mask[idx]:\n",
    "                lines_by_cluster_ph[cluster_key].append(traj_fwd)\n",
    "                lines_by_cluster_ph[cluster_key].append(traj_bwd)\n",
    "\n",
    "\n",
    "# Persistence diagram plotting with error handling\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generating sample persistence diagrams...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "fig_pd, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_idx = 0\n",
    "for topo in [\"closed\", \"open\"]:\n",
    "    if cluster_labels_ph[topo] is None:\n",
    "        continue\n",
    "\n",
    "    labels_ph = cluster_labels_ph[topo]\n",
    "    unique_clusters = np.unique(labels_ph[labels_ph != -1])\n",
    "\n",
    "    for cluster_id in unique_clusters[:3]:\n",
    "        if sample_idx >= 6:\n",
    "            break\n",
    "\n",
    "        mask = labels_ph == cluster_id\n",
    "        indices = np.where(mask)[0]\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            idx = indices[len(indices) // 2]\n",
    "            traj_fwd, traj_bwd = trajectory_pairs[topo][idx]\n",
    "\n",
    "            poincare_pts = compute_poincare_section(traj_fwd, traj_bwd)\n",
    "            ph_features, diagrams = compute_persistent_homology_features(poincare_pts, max_dimension=1)\n",
    "\n",
    "            ax = axes[sample_idx]\n",
    "\n",
    "            # Only plot if we have meaningful diagrams\n",
    "            if len(diagrams[0]) > 0 or (len(diagrams) > 1 and len(diagrams[1]) > 0):\n",
    "                try:\n",
    "                    plot_diagrams(diagrams, ax=ax, legend=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Plot failed for {topo} cluster {cluster_id}: {e}\")\n",
    "                    ax.text(0.5, 0.5, \"No topological features\",\n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"No topological features\",\n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "            ax.set_title(f\"{topo.capitalize()} Cluster {cluster_id}\\n\"\n",
    "                       f\"H1 holes: {ph_features['h1_num_holes']}, \"\n",
    "                       f\"Max persist: {ph_features['h1_max_persistence']:.3f}\",\n",
    "                       fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            sample_idx += 1\n",
    "\n",
    "for i in range(sample_idx, 6):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "pd_file = os.path.join(output_folder, f\"{case}_{step}_persistence_diagrams.png\")\n",
    "fig_pd.savefig(pd_file, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Saved persistence diagrams: {pd_file}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PH-ENHANCED CLUSTERING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
